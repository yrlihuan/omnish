# Example configuration for OpenAI-compatible API with custom URL
# Copy this to ~/.config/omnish/config.toml and customize

[shell]
command = "/bin/bash"
command_prefix = "::"

[daemon]
socket_path = "/tmp/omnish.sock"  # Or use $XDG_RUNTIME_DIR/omnish.sock

[llm]
default = "openai"

# OpenAI-compatible backend with custom API URL
# This works with OpenAI API, Azure OpenAI, or any OpenAI-compatible service
[llm.backends.openai]
backend_type = "openai-compat"
model = "gpt-4"
# Get API key from a command (more secure than hardcoding)
api_key_cmd = "cat ~/.openai_api_key"
# Custom API endpoint (without /chat/completions suffix)
base_url = "https://api.openai.com/v1"

# Alternative: Azure OpenAI example
[llm.backends.azure]
backend_type = "openai-compat"
model = "gpt-4"
api_key_cmd = "echo $AZURE_OPENAI_API_KEY"
base_url = "https://your-resource.openai.azure.com/openai/deployments/gpt-4"

# Alternative: Local LLM with OpenAI-compatible API (e.g., LM Studio, Ollama with openai-compat)
[llm.backends.local]
backend_type = "openai-compat"
model = "llama3"
api_key_cmd = "echo dummy-key"  # Some local servers don't validate the key
base_url = "http://localhost:1234/v1"

# Alternative: Anthropic Claude (for comparison)
[llm.backends.claude]
backend_type = "anthropic"
model = "claude-3-5-sonnet-20241022"
api_key_cmd = "cat ~/.anthropic_api_key"
# base_url is not needed for Anthropic - it uses api.anthropic.com by default

[llm.auto_trigger]
on_nonzero_exit = false
on_stderr_patterns = ["error:", "fatal:", "panic"]
cooldown_seconds = 5
